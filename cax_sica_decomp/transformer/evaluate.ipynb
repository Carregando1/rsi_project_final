{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing cellular automata & optimization classes, and other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(''))))\n",
    "\n",
    "from lamm_automata.blender import Lattice, clear_initial\n",
    "from lamm_automata.ruleset import conway, seeds\n",
    "from lamm_automata.genetic import Optimizer, RulesetMutator, ArbitraryRulesetMutator\n",
    "from lamm_automata.genetic.mutation import ICMutation, SRTMutation\n",
    "from lamm_automata.objectives import surface_to_vol\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "import datetime\n",
    "import decimal\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import transformer modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from x_transformers import Decoder, TransformerWrapper\n",
    "from x_transformers.autoregressive_wrapper import AutoregressiveWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More imports (mostly PyTorch stuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, IterableDataset, DataLoader, get_worker_info\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open all experiment data as Pandas DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_40.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_41.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_6.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_12.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_9.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_14.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_17.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_24.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_49.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_36.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_39.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_31.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_32.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_44.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_45.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_4.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_38.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_7.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_10.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_16.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_48.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_23.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_20.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_43.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_19.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_42.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_0.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_3.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_46.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_28.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_22.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_13.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_5.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_35.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_15.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_47.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_30.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_34.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_21.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_29.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_26.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_2.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_25.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_27.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_18.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_37.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_33.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_11.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_1.h5:\n",
      "EXPERIMENT DATA FROM test_experiment_100ITERS_32GRID_8.h5:\n"
     ]
    }
   ],
   "source": [
    "list_df = []\n",
    "for filename in os.listdir(os.path.join(os.getcwd(), 'data')):\n",
    "    df = pd.read_hdf(f'data/{filename}')\n",
    "    list_df.append(df)\n",
    "    # print data to make sure they are correct\n",
    "    print(f'EXPERIMENT DATA FROM {filename}:')\n",
    "    # with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    #     # print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import tokenizer modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pliam/Desktop/PROJECTS/Automata-Architected-Materials/automata_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers, processors\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from TokenizerChanger import TokenizerChanger\n",
    "from torchtune.generation import sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model we want to open and epoch number\n",
    "model_name = \"test_model_04_12_2029\"\n",
    "\n",
    "model_dir = os.path.join(\"models\", model_name)\n",
    "\n",
    "delimiter_tokens = [\"[BMB]\",\"[ICM]\",\"[SRTM]\",\"[EMB]\"]\n",
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[BOS]\", \"[EOS]\"]\n",
    "# perf_num_tokens = [\"[P]\",\"#\"]\n",
    "perf_num_tokens = [\"[BP]\", \"[PPOS]\", \"[PNEG]\", \"[PEPOS]\", \"[PENEG]\", \"[EP]\"]\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file = os.path.join(model_dir,\"tokenizer.json\"))\n",
    "\n",
    "# special tokens\n",
    "encoder_special_tokens_dict = {\"additional_special_tokens\": delimiter_tokens+special_tokens+perf_num_tokens}\n",
    "tokenizer.add_special_tokens(encoder_special_tokens_dict)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]', 'bos_token': '[BOS]', 'unk_token': '[UNK]', 'eos_token': '[EOS]'})\n",
    "\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_tensor(dat_str):\n",
    "    return torch.Tensor(tokenizer(dat_str, padding=\"longest\", return_tensors=\"pt\")[\"input_ids\"])\n",
    "\n",
    "def tensor_decode(tensor):\n",
    "    if(tensor.dim()==2):\n",
    "        return [tokenizer.decode(seq) for seq in tensor]\n",
    "    return tokenizer.decode(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decimal_to_tokens(val):\n",
    "    formatted_val = \"{:.2e}\".format(val)\n",
    "    digits, exp = formatted_val.split(\"e\")\n",
    "\n",
    "    digits = digits.replace(\".\",\"\")\n",
    "    digits = digits.replace(\"+\",\"\")\n",
    "    digits = digits.replace(\"-\",\"\")\n",
    "\n",
    "    exp = int(exp)\n",
    "    return f\"{'[PPOS]' if val > 0 else '[PNEG]'} {' '.join(digits)} {'[PEPOS]' if exp >= 0 else '[PENEG]'} {abs(exp)}\"\n",
    "\n",
    "data_strings = []\n",
    "initial_ics = []\n",
    "initial_srts = []\n",
    "# performance_metric_vals = []\n",
    "# store performance metric values separately to put them in the input directly, they shouldn't be tokenized\n",
    "for data_frame in list_df:\n",
    "    # final goal: [BOS][BMB][ICM][pos of mutation in IC (2 tokens)][SRTM][pos of mutation in SRT (3 tokens)][P][performance metric after the mutation][EMB] ... [EOS]\n",
    "    data_str = '[BOS] '\n",
    "    # perf_met = []\n",
    "    for index, row in data_frame.iterrows():\n",
    "        # print(row)\n",
    "        if index == 0:\n",
    "            # initial conditions\n",
    "            grid_sz = row[\"ic_cell_pos\"]\n",
    "            initial_ic = row[\"ic_state_old\"]\n",
    "            initial_srt = row[\"srt_state_old\"]\n",
    "            initial_ics.append(initial_ic)\n",
    "            initial_srts.append(initial_srt)\n",
    "            # print(f'grid_sz: {grid_sz}, initial_ic: {initial_ic}, initial_srt: {initial_srt}')\n",
    "        else:\n",
    "            data_str += '[BMB] '\n",
    "            # columns: ic_cell_pos, ic_state_old, ic_state_new, srt_cell_pos, srt_state_old, srt_state_new, objective\n",
    "            ic_mut_strings = []\n",
    "            srt_mut_strings = []\n",
    "            # only include mutation positions since otherwise data will be too big\n",
    "            if type(row[\"ic_cell_pos\"]) != int and len(row[\"ic_cell_pos\"].shape) >= 1:\n",
    "                if len(row[\"ic_cell_pos\"].shape) == 2:\n",
    "                    # batch update\n",
    "                    for i in range(row[\"ic_cell_pos\"].shape[0]):\n",
    "                        ic_mut_strings.append(f'[ICM] {row[\"ic_cell_pos\"][i][0]} {row[\"ic_cell_pos\"][i][1]} ')\n",
    "                else:\n",
    "                    ic_mut_strings.append(f'[ICM] {row[\"ic_cell_pos\"][0]} {row[\"ic_cell_pos\"][1]} ')\n",
    "            \n",
    "            if type(row[\"srt_cell_pos\"]) != int and len(row[\"srt_cell_pos\"].shape) >= 1:\n",
    "                if len(row[\"srt_cell_pos\"].shape) == 2:\n",
    "                    # batch update\n",
    "                    for i in range(row[\"srt_cell_pos\"].shape[0]):\n",
    "                        ic_mut_strings.append(f'[SRTM] {row[\"srt_cell_pos\"][i][0]} {row[\"srt_cell_pos\"][i][1]} {row[\"srt_cell_pos\"][i][2]} ')\n",
    "                else:\n",
    "                    ic_mut_strings.append(f'[SRTM] {row[\"srt_cell_pos\"][0]} {row[\"srt_cell_pos\"][1]} {row[\"srt_cell_pos\"][2]} ')\n",
    "            data_str += ''.join(ic_mut_strings)\n",
    "            data_str += ''.join(srt_mut_strings)\n",
    "            data_str += f'[BP] {decimal_to_tokens(row[\"objective\"])} [EP] '\n",
    "            data_str += '[EMB] '\n",
    "    data_str += '[EOS]'\n",
    "\n",
    "    data_strings.append(data_str)\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "encoded = str_to_tensor(data_strings)\n",
    "# print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model test_model_04_12_2029 in epoch number 1\n",
      "model hyperparams: {'model_name': 'test_model', 'num_tokens': 93, 'max_seq_len': 100, 'dim': 50, 'depth': 6, 'limit_seq_len': 100, 'heads': 4, 'rotary_pos_emb': True, 'attn_flash': True, 'masking': False, 'mask_prob': 0.15}\n",
      "model training params: {'epochs': 1, 'batch_size': 1, 'lr': 0.001, 'workers': 0, 'drop_last': False}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AutoregressiveWrapper(\n",
       "  (net): TransformerWrapper(\n",
       "    (token_emb): TokenEmbedding(\n",
       "      (emb): Embedding(93, 50)\n",
       "    )\n",
       "    (post_emb_norm): Identity()\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (project_emb): Identity()\n",
       "    (attn_layers): Decoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): LayerNorm(\n",
       "              (ln): LayerNorm((50,), eps=1e-05, elementwise_affine=False)\n",
       "            )\n",
       "            (1-2): 2 x None\n",
       "          )\n",
       "          (1): Attention(\n",
       "            (to_q): Linear(in_features=50, out_features=256, bias=False)\n",
       "            (to_k): Linear(in_features=50, out_features=256, bias=False)\n",
       "            (to_v): Linear(in_features=50, out_features=256, bias=False)\n",
       "            (split_q_heads): Rearrange('b n (h d) -> b h n d', h=4)\n",
       "            (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
       "            (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
       "            (merge_heads): Rearrange('b h n d -> b n (h d)')\n",
       "            (attend): Attend(\n",
       "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (to_out): Linear(in_features=256, out_features=50, bias=False)\n",
       "          )\n",
       "          (2): Residual()\n",
       "        )\n",
       "        (1): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): LayerNorm(\n",
       "              (ln): LayerNorm((50,), eps=1e-05, elementwise_affine=False)\n",
       "            )\n",
       "            (1-2): 2 x None\n",
       "          )\n",
       "          (1): FeedForward(\n",
       "            (ff): Sequential(\n",
       "              (0): Sequential(\n",
       "                (0): Linear(in_features=50, out_features=200, bias=True)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "              (2): Linear(in_features=200, out_features=50, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Residual()\n",
       "        )\n",
       "        (2): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): LayerNorm(\n",
       "              (ln): LayerNorm((50,), eps=1e-05, elementwise_affine=False)\n",
       "            )\n",
       "            (1-2): 2 x None\n",
       "          )\n",
       "          (1): Attention(\n",
       "            (to_q): Linear(in_features=50, out_features=256, bias=False)\n",
       "            (to_k): Linear(in_features=50, out_features=256, bias=False)\n",
       "            (to_v): Linear(in_features=50, out_features=256, bias=False)\n",
       "            (split_q_heads): Rearrange('b n (h d) -> b h n d', h=4)\n",
       "            (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
       "            (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
       "            (merge_heads): Rearrange('b h n d -> b n (h d)')\n",
       "            (attend): Attend(\n",
       "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (to_out): Linear(in_features=256, out_features=50, bias=False)\n",
       "          )\n",
       "          (2): Residual()\n",
       "        )\n",
       "        (3): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): LayerNorm(\n",
       "              (ln): LayerNorm((50,), eps=1e-05, elementwise_affine=False)\n",
       "            )\n",
       "            (1-2): 2 x None\n",
       "          )\n",
       "          (1): FeedForward(\n",
       "            (ff): Sequential(\n",
       "              (0): Sequential(\n",
       "                (0): Linear(in_features=50, out_features=200, bias=True)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "              (2): Linear(in_features=200, out_features=50, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Residual()\n",
       "        )\n",
       "        (4): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): LayerNorm(\n",
       "              (ln): LayerNorm((50,), eps=1e-05, elementwise_affine=False)\n",
       "            )\n",
       "            (1-2): 2 x None\n",
       "          )\n",
       "          (1): Attention(\n",
       "            (to_q): Linear(in_features=50, out_features=256, bias=False)\n",
       "            (to_k): Linear(in_features=50, out_features=256, bias=False)\n",
       "            (to_v): Linear(in_features=50, out_features=256, bias=False)\n",
       "            (split_q_heads): Rearrange('b n (h d) -> b h n d', h=4)\n",
       "            (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
       "            (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
       "            (merge_heads): Rearrange('b h n d -> b n (h d)')\n",
       "            (attend): Attend(\n",
       "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (to_out): Linear(in_features=256, out_features=50, bias=False)\n",
       "          )\n",
       "          (2): Residual()\n",
       "        )\n",
       "        (5): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): LayerNorm(\n",
       "              (ln): LayerNorm((50,), eps=1e-05, elementwise_affine=False)\n",
       "            )\n",
       "            (1-2): 2 x None\n",
       "          )\n",
       "          (1): FeedForward(\n",
       "            (ff): Sequential(\n",
       "              (0): Sequential(\n",
       "                (0): Linear(in_features=50, out_features=200, bias=True)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "              (2): Linear(in_features=200, out_features=50, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Residual()\n",
       "        )\n",
       "        (6): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): LayerNorm(\n",
       "              (ln): LayerNorm((50,), eps=1e-05, elementwise_affine=False)\n",
       "            )\n",
       "            (1-2): 2 x None\n",
       "          )\n",
       "          (1): Attention(\n",
       "            (to_q): Linear(in_features=50, out_features=256, bias=False)\n",
       "            (to_k): Linear(in_features=50, out_features=256, bias=False)\n",
       "            (to_v): Linear(in_features=50, out_features=256, bias=False)\n",
       "            (split_q_heads): Rearrange('b n (h d) -> b h n d', h=4)\n",
       "            (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
       "            (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
       "            (merge_heads): Rearrange('b h n d -> b n (h d)')\n",
       "            (attend): Attend(\n",
       "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (to_out): Linear(in_features=256, out_features=50, bias=False)\n",
       "          )\n",
       "          (2): Residual()\n",
       "        )\n",
       "        (7): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): LayerNorm(\n",
       "              (ln): LayerNorm((50,), eps=1e-05, elementwise_affine=False)\n",
       "            )\n",
       "            (1-2): 2 x None\n",
       "          )\n",
       "          (1): FeedForward(\n",
       "            (ff): Sequential(\n",
       "              (0): Sequential(\n",
       "                (0): Linear(in_features=50, out_features=200, bias=True)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "              (2): Linear(in_features=200, out_features=50, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Residual()\n",
       "        )\n",
       "        (8): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): LayerNorm(\n",
       "              (ln): LayerNorm((50,), eps=1e-05, elementwise_affine=False)\n",
       "            )\n",
       "            (1-2): 2 x None\n",
       "          )\n",
       "          (1): Attention(\n",
       "            (to_q): Linear(in_features=50, out_features=256, bias=False)\n",
       "            (to_k): Linear(in_features=50, out_features=256, bias=False)\n",
       "            (to_v): Linear(in_features=50, out_features=256, bias=False)\n",
       "            (split_q_heads): Rearrange('b n (h d) -> b h n d', h=4)\n",
       "            (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
       "            (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
       "            (merge_heads): Rearrange('b h n d -> b n (h d)')\n",
       "            (attend): Attend(\n",
       "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (to_out): Linear(in_features=256, out_features=50, bias=False)\n",
       "          )\n",
       "          (2): Residual()\n",
       "        )\n",
       "        (9): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): LayerNorm(\n",
       "              (ln): LayerNorm((50,), eps=1e-05, elementwise_affine=False)\n",
       "            )\n",
       "            (1-2): 2 x None\n",
       "          )\n",
       "          (1): FeedForward(\n",
       "            (ff): Sequential(\n",
       "              (0): Sequential(\n",
       "                (0): Linear(in_features=50, out_features=200, bias=True)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "              (2): Linear(in_features=200, out_features=50, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Residual()\n",
       "        )\n",
       "        (10): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): LayerNorm(\n",
       "              (ln): LayerNorm((50,), eps=1e-05, elementwise_affine=False)\n",
       "            )\n",
       "            (1-2): 2 x None\n",
       "          )\n",
       "          (1): Attention(\n",
       "            (to_q): Linear(in_features=50, out_features=256, bias=False)\n",
       "            (to_k): Linear(in_features=50, out_features=256, bias=False)\n",
       "            (to_v): Linear(in_features=50, out_features=256, bias=False)\n",
       "            (split_q_heads): Rearrange('b n (h d) -> b h n d', h=4)\n",
       "            (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
       "            (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
       "            (merge_heads): Rearrange('b h n d -> b n (h d)')\n",
       "            (attend): Attend(\n",
       "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (to_out): Linear(in_features=256, out_features=50, bias=False)\n",
       "          )\n",
       "          (2): Residual()\n",
       "        )\n",
       "        (11): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): LayerNorm(\n",
       "              (ln): LayerNorm((50,), eps=1e-05, elementwise_affine=False)\n",
       "            )\n",
       "            (1-2): 2 x None\n",
       "          )\n",
       "          (1): FeedForward(\n",
       "            (ff): Sequential(\n",
       "              (0): Sequential(\n",
       "                (0): Linear(in_features=50, out_features=200, bias=True)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "              (2): Linear(in_features=200, out_features=50, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Residual()\n",
       "        )\n",
       "      )\n",
       "      (layer_integrators): ModuleList(\n",
       "        (0-11): 12 x None\n",
       "      )\n",
       "      (rotary_pos_emb): RotaryEmbedding()\n",
       "      (adaptive_mlp): Identity()\n",
       "      (final_norm): LayerNorm(\n",
       "        (ln): LayerNorm((50,), eps=1e-05, elementwise_affine=False)\n",
       "      )\n",
       "      (skip_combines): ModuleList(\n",
       "        (0-11): 12 x None\n",
       "      )\n",
       "    )\n",
       "    (to_logits): Linear(in_features=50, out_features=93, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_num = 1\n",
    "print(f\"Loading model {model_name} in epoch number {epoch_num}\")\n",
    "\n",
    "# load model hyperparameters\n",
    "with open(os.path.join(model_dir,'model_hyperparams.json')) as f:\n",
    "    d = json.load(f)\n",
    "    print(f\"model hyperparams: {d}\")\n",
    "    num_tokens = d[\"num_tokens\"]\n",
    "    max_seq_len = d[\"max_seq_len\"]\n",
    "    DIM = d[\"dim\"]\n",
    "    DEPTH = d[\"depth\"]\n",
    "    HEADS = d[\"heads\"]\n",
    "    ATTN_FLASH = d[\"attn_flash\"]\n",
    "    ROTARY_POS_EMB = d[\"rotary_pos_emb\"]\n",
    "\n",
    "with open(os.path.join(model_dir,'model_training_params.json')) as f:\n",
    "    d = json.load(f)\n",
    "    print(f\"model training params: {d}\")\n",
    "\n",
    "model = TransformerWrapper(\n",
    "    num_tokens = num_tokens,\n",
    "    max_seq_len = max_seq_len,\n",
    "    attn_layers = Decoder(\n",
    "        dim = DIM,\n",
    "        depth = DEPTH,\n",
    "        heads = HEADS,\n",
    "        attn_flash = ATTN_FLASH,\n",
    "        rotary_pos_emb = ROTARY_POS_EMB,\n",
    "    ), \n",
    ")\n",
    "# wrap the transformer into an autoregressor\n",
    "model = AutoregressiveWrapper(model)\n",
    "\n",
    "# load the model\n",
    "model.load_state_dict(torch.load(os.path.join(model_dir, f\"weights_{epoch_num}.pt\"), weights_only=True))\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer-Powered Genetic Algorithm Performance Evaluation & Comparison with Dataset Sequences on Randomly Generated Cellular Automata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8850/1118734386.py:188: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  decoded = tensor_decode(torch.tensor(token, dtype=torch.int8))\n",
      "/tmp/ipykernel_8850/1118734386.py:203: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  decoded = tensor_decode(torch.tensor(token, dtype=torch.int8))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERFORMANCE COMPARISON: [{'model_perf_improvs': [-0.044068912901405355, -0.007574957801577575], 'avg_model_improv': -0.017214623567660976, 'data_perf_improvs': [-0.023274473284094377, -0.007173798239308304, -0.0318346823092206, -0.02760949071494423, -0.012920683534611399, -0.004234297515270491, -0.01398144397686174, -0.07412339637322063, -0.06690008805748526, -0.041100105801689324, -0.00475752413531616, -0.03044045037737053, -0.045496229340044714, -0.002796427288607184, -0.00855516128332301, -0.01591174864868261, -0.0028485949240408814, -0.012236114673535248, -0.04084392779582657, -0.01783995405248895, -0.006627017184237971, -0.0017586451684348248, -0.07026247771835958, -0.0017851857767672286, -0.043866141219711, -0.012609093043772113, -0.0021991302037065452, -0.03467433066428249, -0.008965340028307622, -0.034461397628873236, -0.011533697376996166, -0.02374921800110208, -0.07248760921504704, -0.014405228758169741, -0.005781424234284849, -0.05852704613621995, -0.05799978097793357, -0.00038819875776407997, -0.00023037136855474927, -0.04144798118493931, -0.050343272224568025, -0.04197645300399078, -0.02570548629281255, -0.01760824479012335, -0.03869094224619829, -0.03586247736048076, -0.0007771241941600948, -0.027385844592236808, -0.05808743819067885, -0.004592561600770928, -0.007746600425821093], 'avg_data_improv': -0.001297414351895248}, {'model_perf_improvs': [-0.0189247173216458, -0.006067711714364599], 'avg_model_improv': -0.0124962145180052, 'data_perf_improvs': [-0.039659969270239515, -0.0016612642529416632, -0.0019271876857098746, -0.0016680478802326704, -0.004733392494302713, -0.007334773492752689, -0.004499963684223118, -0.0066269902348095044, -0.02384997838405578, -0.001777292201351699, -0.0071972943108820076, -0.006945827218399536, -0.031693165328894324, -0.03345390056093933, -0.05376701345682644, -0.00968970143841208, -0.017360085822432758, -0.029821931711268235, -0.00889757845182615, -0.0035815687011959696, -0.002936857562407802, -0.005709070478503442, -0.030025170418033653, -0.003120149125757088, -0.009770284808474194, -0.007427206311700019, -0.0006065418145642099, -0.005406880199337216, -0.031737362342528996, -0.013124763435322429, -0.005967601500406872, -0.005508068668969912, -0.010552173139448229, -0.03736789291475162, -0.02437757851163802, -0.00814711916492783, -0.017275369973571486, -0.007476577309663668, -0.018189030700241027, -0.03366574282277934, -0.0007542973368677153, -0.023754487345727604, -0.04760139065908309, -0.001090422237050248, -0.039175334255987515, -7.443869025181016e-06, -0.013800711361189322, -0.013285505961952992, -0.004300458715595923, -0.04193883610451277], 'avg_data_improv': -0.0007602472556317155}, {'model_perf_improvs': [-0.005515884052726072], 'avg_model_improv': -0.002757942026363036, 'data_perf_improvs': [-0.015067823365883193, -0.01243657403170495, -0.007368816913563947, -0.04270744831492479, -0.0027442050563442066, -0.021412165956011364, -0.002256162774771475, -0.04539372997751201, -0.054915393198257334, -0.0483870616778912, -0.03105508051458461, -0.0027825294837562353, -0.025433960198521177, -0.0077253665012015915, -0.00032672945134937237, -0.006781915853339093, -0.06203586764334457, -0.014845442479351156, -0.017544193566542354, -0.009755824103661048, -0.012865880026002863, -0.014156890430320423, -0.016194323579234116, -0.021251109395466372, -0.01025021972948359, -0.005045821427199115, -0.009258441372375081, -0.020100733802721926, -0.00336299879459645, -0.005019271732852104, -0.00563084845841022, -0.020050514127873953, -0.005118768819003527, -0.03883609851659564], 'avg_data_improv': -0.0006181182112746511}, {'model_perf_improvs': [-0.01132654164717728, -0.026631058797861762], 'avg_model_improv': -0.00474470005562988, 'data_perf_improvs': [-0.03042625965239676, -0.018215488851957673, -0.018289677652439273, -0.004418878863949871, -0.024952052754496634, -0.010733044856690022, -0.004497393894266288, -0.01177010754634189, -0.007527319944079736, -0.0065467684972446705, -0.022690792375957614, -0.002534022688426063, -0.017200288223853555, -0.005490063760718478, -0.02435532217672698, -0.02358890555208326, -0.025928132992954822, -0.04567427629252929, -0.0026973389900524225, -0.03282758997301283, -0.043764227250977505, -0.03486509698609819, -0.012005955148679526, -0.014170115971212205, -0.004896560166482544, -0.01557447330399242, -0.0022030478798136244, -0.009308343854779189, -0.04685632801662276, -0.0009159570551435792, -0.014262603712829502, -0.03199954732111099, -0.0005358776908002127, -0.015210034676247375], 'avg_data_improv': -0.0005869318945749678}, {'model_perf_improvs': [], 'avg_model_improv': 0.0, 'data_perf_improvs': [-0.025826772501202022, -0.004559220943501252, -0.020133171433626806, -0.007231492294710762, -0.022741881611716863, -0.006407678981923404, -0.02593232490364361, -0.0016010314821732763, -0.0071824743126889246, -0.03627303071616872, -0.001685690506842974, -0.01965572421727657, -0.002682463979534866], 'avg_data_improv': -0.00018191295788501006}, {'model_perf_improvs': [], 'avg_model_improv': 0.0, 'data_perf_improvs': [-0.006298708231023298, -0.017608212283808378, -0.008620917070863321, -0.026130289478808244, -0.01639450625251815, -0.023405399546521544, -0.003860725758819683, -0.06244668911335527, -0.009191885875595496, -0.014718381573207928, -0.01947616994487955, -0.014466229919021245, -0.018926590117404274, -0.0006522577508007998, -0.019024552692924246, -0.0007349552071200804, -0.001055027264862396, -0.003782849086082507, -0.023464952241657677, -0.005859590147485605, -0.031506464463710415, -0.003287724894373234, -0.008578798233970453, -0.010703640603869857, -0.03437088269869282, -0.009874893304766985, -0.04527367115828529, -0.0018187070681134898, -0.020760008128429597, -0.008710345715198464, -0.007030455209179998, -0.017229823614993478, -0.012212619330056462, -0.010774720301756524, -0.01907474441726098, -0.009960596713909808, -0.00564142209245766, -0.0386153455709346, -0.017227095342243537, -0.005040913752409892, -0.03554440340444032, -0.029753470000895454, -0.004245725486989649, -0.001238683098934601, -0.03781670237688317, -0.010096467781322005, -0.023034473294713642, -0.0450058745773152, -0.01228769886551806, -0.005340054099516323, -0.02230904563664904], 'avg_data_improv': -0.0008404843607945507}, {'model_perf_improvs': [-0.02287346867321638], 'avg_model_improv': -0.0076244895577387934, 'data_perf_improvs': [-0.008089599963149396, -0.021289378619655075, -0.02372821325358032, -0.008903827604549441, -0.06256222392893562, -0.001943575758561522, -0.01765653886296281, -0.07977228369341738, -0.04549097048356199, -0.005417349221383816, -0.022060687850161464, -0.011285053860599525, -0.0012920822838307089, -0.013918373641059745, -0.004069906245362631, -0.0496150492520826, -0.00277225463939601, -0.04909139388973127, -0.04709949148757797, -0.010564733256535064, -0.012385338094636822, -0.026138821687040625, -0.0015085484411665462, -0.014407164644039128, -0.04474079331002123, -0.008865893305217476, -0.031653865239223755, -0.005424482784510687, -0.018174712201306065, -0.0272753776030914, -0.07200827238384555, -0.000948108159998462, -0.016145932448496936, -0.015188678815178491, -0.00031159291747240303], 'avg_data_improv': -0.0007818005698313399}, {'model_perf_improvs': [-0.022094363104796955, -0.009837730862702898, -0.006656354336374015], 'avg_model_improv': -0.006431408050645644, 'data_perf_improvs': [-0.016835155782219502, -0.042143446515031435, -0.0341868135355714, -0.027968477808526515, -0.023603974119592586, -0.003442167027220222, -0.03265707656431882, -0.007200418672632658, -0.012081882752475792, -0.023056736048063797, -0.004886638408490462, -0.0034136644054632015, -0.006901923112202724, -0.014587951954960765, -0.06480494145692361, -0.001314086219746713, -0.04433441318501341, -0.010595436839175854, -0.00013201320132072425, -0.1011893553247809, -0.03459847819937245, -0.016276969492467153, -0.00854691621429371, -0.057844804062003874, -0.009733800901209122, -0.01871976436911904, -0.016386335668071794, -0.016762374763996668, -0.0005728590297069402, -0.007323387504781564, -0.0016925937684248993, -0.02238766781828172, -5.031795355758817e-05, -0.005067426225811467, -0.018734897203152023, -0.0034083425170248205, -0.0017649368774028673, -0.021056466302368015, -0.02143574398756254, -0.0010923784246257284, -0.07884660361348672, -0.017678543955106818, -0.018240598337958325, -0.019778558447081096, -0.02365335448992134, -0.007626858647321377, -0.016214981262687722, -0.04997241608329883, -0.02806391605558023, -0.008072707236257415, -0.012052487063047401, -0.026211971829471103, -0.016053392466026217, -0.021068333436661035, -0.015782094466842977, -0.08168879106277149, -0.009648543982466506, -0.03396857017716837, -0.08209773310088586, -0.004898223608772234, -0.0003954171903215453, -0.008683457487853374], 'avg_data_improv': -0.0013394905882179531}, {'model_perf_improvs': [-0.03318757342383449, -0.01490161533301304, -0.027642790702468112, -0.02182861666336766], 'avg_model_improv': -0.01626009935378055, 'data_perf_improvs': [-0.04799160477881781, -0.0065051517683096804, -0.03239216760467567, -0.00494293452809913, -0.06549647807316461, -0.027204039120150014, -0.0018878090857263885, -0.007557076167599597, -0.0005115769619346011, -0.03288136900769789, -0.018801802553994662, -0.0005303374678007344, -0.033249808082540966, -0.0020197268411781266, -0.010450390106551133, -0.016415047039407327, -0.04761593676250797, -0.017175750586760685, -0.00183165635225091, -0.000976605850756762, -0.006033567027830422, -0.008085757871747035, -0.01867046427204766, -0.06140634048150595, -0.023965684573867385, -0.010978158845716735, -0.028210371441752535, -0.00022797655733697297, -0.019277791622874574, -0.015348856259342902, -0.01700717134265428, -0.01843575779119, -0.0277972707859675, -0.03509576532832348, -0.003008147231254199, -0.01145307737653134, -0.03440394975156025, -0.06784059434133205, -0.007154312049449274, -0.015052534971424336, -0.04326851181520297, -0.05478974155997651, -0.005339469805139885, -0.0513755560127267, -0.01150037670864279, -0.0008603368528925515, -0.012280928633097155, -0.009434017384800875, -0.011758485434269161, -0.0010944449901080233, -0.03590706196652693], 'avg_data_improv': -0.0010434997498270171}, {'model_perf_improvs': [-0.00656749487377617], 'avg_model_improv': -0.00656749487377617, 'data_perf_improvs': [-0.0812784406004905, -0.001193426253177421, -0.008336818178172223, -0.014650393472075862, -0.01373886246399092, -0.036283976838683785, -0.01904267706953, -0.018041776895944217, -0.008268394471913965, -0.023425180198830553, -0.0040656511478225354, -0.008727142620346129, -0.01771884986124217, -0.02866171349220714, -0.002870813397128913, -0.01866258741258786, -0.017913950824109293, -0.012973285500079257, -0.012073078097300183, -0.0034489124134440274, -0.036870508022724735, -0.0012617706324009603, -0.015059819172577704, -0.0004756406625556764, -0.007606739566969445, -0.01982033665435612, -0.05141789112768258, -0.0030088784146320435, -0.0002010790043005528, -0.0027977250821296096, -0.007819458494712883, -0.024183488291030564, -0.00951519611130447, -0.04816965371769033, -0.01401072983495144, -0.019335771356404408, -0.012478141434704604, -0.02397683617195767, -0.008282907699699749, -0.08062086639689969, -0.0009660458863125498, -0.01837178900913461, -0.03733841708109775, -0.02185544524320715, -0.004514692986060886, -0.005044743750296732, -0.07533363205257437, -0.01327137674491663, -0.02056819321985337, -0.011084369650617987, -0.0008314279405050584, -0.0005036668253817211, -0.00257588544894638, -0.0011715048287470609, -0.01614272578039433, -0.01068765029605423, -0.018251480476865645, -0.0013912818937464877, -0.036617548123873966, -0.001615936241511129, -0.003949028373448016, -0.0041463893638686855, -0.006688323250383377, -0.01584948391468277, -0.003425639723188212, -0.047598392009183144], 'avg_data_improv': -0.0011180844391736145}]\n"
     ]
    }
   ],
   "source": [
    "NUM_TEST_EXPERIMENTS = 10\n",
    "NUM_TEST_SEQUENCES = 10\n",
    "GRID_SIZE = 32\n",
    "RULESET_MUTATOR_CLASS = ArbitraryRulesetMutator\n",
    "RULE_SET = [conway(), seeds()]\n",
    "OPT_FUNC = surface_to_vol\n",
    "SRT_NUM_MUTATE = 10\n",
    "IC_NUM_MUTATE = 10\n",
    "RULESET_MUTE_PROB = 2/3\n",
    "\n",
    "def export(lattice: Lattice, iteration: int, export_dir: str):\n",
    "    stlfile = os.path.join(export_dir, \"genetic%.5d.stl\" % iteration)\n",
    "    lattice.clear_lattice()\n",
    "    lattice.update_selected(optim.state.generate())\n",
    "    lattice.export_stl(stlfile)\n",
    "    with open(os.path.join(export_dir, \"objective.txt\"), \"a\", encoding=\"utf-8\") as w:\n",
    "        w.write(f\"{iteration}: {optim.objvalue}\\n\")\n",
    "\n",
    "\n",
    "# Measure the difference between the resulting performance metrics when applying the predicted mutations and running the automaton\n",
    "\n",
    "performance_logs = []\n",
    "\n",
    "for num_test in range(NUM_TEST_EXPERIMENTS):\n",
    "    # clear_initial()\n",
    "    # lattice = Lattice(dim=(GRID_SIZE, GRID_SIZE, GRID_SIZE))\n",
    "\n",
    "    ruleset_mutator_init = RULESET_MUTATOR_CLASS(rules=RULE_SET, grid_size=GRID_SIZE, mutate_p=1/(GRID_SIZE**2) * (SRT_NUM_MUTATE+IC_NUM_MUTATE), rule_mutate_p=RULESET_MUTE_PROB)\n",
    "\n",
    "    optim_init = Optimizer(mutator=ruleset_mutator_init, objective=lambda grid: OPT_FUNC(grid))\n",
    "\n",
    "    model_perf_improvs = []\n",
    "    model_perf_sum = 0\n",
    "    model_mut_batch_num = 0\n",
    "    data_perf_improvs = []\n",
    "    data_perf_sum = 0\n",
    "    data_mut_batch_num = 0\n",
    "    \n",
    "    # test the mutation prediction model\n",
    "    for repeat_num in range(NUM_TEST_SEQUENCES):\n",
    "        optim = deepcopy(optim_init)\n",
    "        \n",
    "        curr_level = \"seq\" # seq, mutb, icm, srtm\n",
    "        curr_output = torch.full((1,max_seq_len), int(str_to_tensor(\"[PAD]\")[0]))\n",
    "        curr_idx = 1\n",
    "        in_idx = 0\n",
    "        curr_output[0][0] = str_to_tensor(\"[BOS]\")[0]\n",
    "        ic_mutations, srt_mutations = [], []\n",
    "        curr_mutation = None\n",
    "        it = 0\n",
    "\n",
    "        while curr_idx < max_seq_len:\n",
    "            with torch.no_grad():\n",
    "                next_output = model.net(curr_output.cuda())\n",
    "                # print(model.net.num_tokens)\n",
    "                # print(next_output.shape)\n",
    "                logits = next_output[0][curr_idx]\n",
    "                # print(logits)\n",
    "            valid = False\n",
    "            added_perf = False\n",
    "            while valid == False:\n",
    "                token = int(sample(logits)[0])\n",
    "                # print(token)\n",
    "                if curr_level == \"seq\":\n",
    "                    if token == int(str_to_tensor(\"[BMB]\")[0]):\n",
    "                        # print('STARTING MUTATION BATCH')\n",
    "                        curr_level = \"mutb\"\n",
    "                        valid = True\n",
    "                elif curr_level == \"mutb\":\n",
    "                    if token == int(str_to_tensor(\"[EMB]\")[0]):\n",
    "                        # perform mutations predicted & add the resulting performance to the sequence\n",
    "                        # print(f\"MUTATION BATCH {it}: IC: {ic_mutations}, SRT: {srt_mutations}\")\n",
    "                        old_objval = optim.objvalue\n",
    "                        accepted, new_state = optim.step_muts(ic_mutations, srt_mutations)\n",
    "                        if accepted:\n",
    "                            # print(\"Got a better state!\", optim.objvalue)\n",
    "                            model_perf_sum += optim.objvalue - old_objval\n",
    "                            model_perf_improvs.append(optim.objvalue - old_objval)\n",
    "                            # export(it + 1, lattice, export_dir)\n",
    "                        perf_tokens = str_to_tensor(f'[BP] {decimal_to_tokens(optim.objvalue)} [EP] ')[0]\n",
    "                        added_perf = True\n",
    "                        it += 1\n",
    "                        model_mut_batch_num += 1\n",
    "                        # end mutation batch\n",
    "                        curr_level = \"seq\"\n",
    "                        valid = True\n",
    "                        ic_mutations, srt_mutations = [], []\n",
    "                    elif token == int(str_to_tensor(\"[ICM]\")[0]):\n",
    "                        # got ic mutation\n",
    "                        curr_mutation = [0, 0]\n",
    "                        curr_level = \"icm\"\n",
    "                        valid = True\n",
    "                    elif token == int(str_to_tensor(\"[SRTM]\")[0]):\n",
    "                        # got srt mutation\n",
    "                        curr_mutation = [0, 0, 0]\n",
    "                        curr_level = \"srtm\"\n",
    "                        valid = True\n",
    "                elif curr_level == \"icm\":\n",
    "                    decoded = tensor_decode(torch.tensor(token, dtype=torch.int8))\n",
    "                    if decoded.isdigit() and int(decoded) < GRID_SIZE:\n",
    "                        # got ic dimension\n",
    "                        curr_mutation[in_idx] = int(decoded)\n",
    "                        in_idx += 1\n",
    "                        valid = True\n",
    "                        if in_idx == 2:\n",
    "                            in_idx = 0\n",
    "                            curr_level = \"mutb\"\n",
    "                            # print(f\"ICM: {curr_mutation}\")\n",
    "                            ic_mutations.append(curr_mutation)\n",
    "                            curr_mutation = None\n",
    "\n",
    "                elif curr_level == \"srtm\":\n",
    "                    decoded = tensor_decode(torch.tensor(token, dtype=torch.int8))\n",
    "                    if decoded.isdigit() and (in_idx > 0 or int(decoded) < GRID_SIZE-1):\n",
    "                        # got srt dimension\n",
    "                        curr_mutation[in_idx] = int(decoded)\n",
    "                        in_idx += 1\n",
    "                        valid = True\n",
    "                        if in_idx == 3:\n",
    "                            in_idx = 0\n",
    "                            curr_level = \"mutb\"\n",
    "                            # print(f\"SRTM: {curr_mutation}\")\n",
    "                            srt_mutations.append(curr_mutation)\n",
    "                            curr_mutation = None\n",
    "            curr_output[0][curr_idx] = token\n",
    "            curr_idx += 1\n",
    "            if added_perf and (curr_idx + perf_tokens.shape[0] <= 100):\n",
    "                for i in range(perf_tokens.shape[0]):\n",
    "                    curr_output[0][curr_idx] = perf_tokens[i]\n",
    "                    curr_idx += 1\n",
    "            \n",
    "    # test the dataset predictions\n",
    "    for repeat_num in random.sample(range(len(encoded)), NUM_TEST_SEQUENCES):\n",
    "        optim = deepcopy(optim_init)\n",
    "\n",
    "        curr_level = \"seq\" # seq, mutb, icm, srtm, perf\n",
    "        curr_idx = 1\n",
    "        in_idx = 0\n",
    "        ic_mutations, srt_mutations = [], []\n",
    "        curr_mutation = None\n",
    "        it = 0\n",
    "\n",
    "        while curr_idx < len(encoded[repeat_num]):\n",
    "            valid = False\n",
    "            token = encoded[repeat_num][curr_idx]\n",
    "            # print(tensor_decode(torch.tensor(token, dtype=torch.int8)))\n",
    "            if curr_level == \"seq\":\n",
    "                if token == int(str_to_tensor(\"[BMB]\")[0]):\n",
    "                    # print('STARTING MUTATION BATCH')\n",
    "                    curr_level = \"mutb\"\n",
    "                    valid = True\n",
    "                elif token == int(str_to_tensor(\"[EOS]\")[0]):\n",
    "                    break\n",
    "            elif curr_level == \"mutb\":\n",
    "                if token == int(str_to_tensor(\"[EMB]\")[0]):\n",
    "                    # perform mutations predicted & add the resulting performance to the sequence\n",
    "                    # print(f\"MUTATION BATCH {it}: IC: {ic_mutations}, SRT: {srt_mutations}\")\n",
    "                    old_objval = optim.objvalue\n",
    "                    accepted, new_state = optim.step_muts(ic_mutations, srt_mutations)\n",
    "                    if accepted:\n",
    "                        # print(\"Got a better state!\", optim.objvalue)\n",
    "                        data_perf_sum += optim.objvalue - old_objval\n",
    "                        data_perf_improvs.append(optim.objvalue - old_objval)\n",
    "                        # export(it + 1, lattice, export_dir)\n",
    "                    it += 1\n",
    "                    data_mut_batch_num += 1\n",
    "                    # end mutation batch\n",
    "                    curr_level = \"seq\"\n",
    "                    valid = True\n",
    "                    ic_mutations, srt_mutations = [], []\n",
    "                elif token == int(str_to_tensor(\"[ICM]\")[0]):\n",
    "                    # got ic mutation\n",
    "                    # print(\"GOT IC MUTATION\")\n",
    "                    curr_mutation = [0, 0]\n",
    "                    curr_level = \"icm\"\n",
    "                    valid = True\n",
    "                elif token == int(str_to_tensor(\"[SRTM]\")[0]):\n",
    "                    # got srt mutation\n",
    "                    # print(\"GOT SRT MUTATION\")\n",
    "                    curr_mutation = [0, 0, 0]\n",
    "                    curr_level = \"srtm\"\n",
    "                    valid = True\n",
    "                elif token == int(str_to_tensor(\"[BP]\")[0]):\n",
    "                    # print(\"GOT PERF\")\n",
    "                    curr_level = \"perf\"\n",
    "                    valid = True\n",
    "            elif curr_level == \"icm\":\n",
    "                decoded = tensor_decode(torch.tensor(token, dtype=torch.int8))\n",
    "                if decoded.isdigit() and int(decoded) < GRID_SIZE:\n",
    "                    # print(\"GOT IC DIM\")\n",
    "                    # got ic dimension\n",
    "                    curr_mutation[in_idx] = int(decoded)\n",
    "                    in_idx += 1\n",
    "                    valid = True\n",
    "                    if in_idx == 2:\n",
    "                        in_idx = 0\n",
    "                        curr_level = \"mutb\"\n",
    "                        # print(f\"ICM: {curr_mutation}\")\n",
    "                        ic_mutations.append(curr_mutation)\n",
    "                        curr_mutation = None\n",
    "\n",
    "            elif curr_level == \"srtm\":\n",
    "                decoded = tensor_decode(torch.tensor(token, dtype=torch.int8))\n",
    "                if decoded.isdigit() and (in_idx > 0 or int(decoded) < GRID_SIZE-1):\n",
    "                    # print(\"GOT SRT DIM\")\n",
    "                    # got srt dimension\n",
    "                    curr_mutation[in_idx] = int(decoded)\n",
    "                    in_idx += 1\n",
    "                    valid = True\n",
    "                    if in_idx == 3:\n",
    "                        in_idx = 0\n",
    "                        curr_level = \"mutb\"\n",
    "                        # print(f\"SRTM: {curr_mutation}\")\n",
    "                        srt_mutations.append(curr_mutation)\n",
    "                        curr_mutation = None\n",
    "            \n",
    "            elif curr_level == \"perf\":\n",
    "                if token == int(str_to_tensor(\"[EP]\")[0]):\n",
    "                    # print(\"ENDED PERF\")\n",
    "                    curr_level = \"mutb\"\n",
    "                valid = True\n",
    "            assert valid, \"NOT VALID DATASET SEQUENCE\"\n",
    "            curr_idx += 1\n",
    "    \n",
    "    performance_logs.append({\"model_perf_improvs\": model_perf_improvs, \"avg_model_improv\": float(model_perf_sum)/model_mut_batch_num if model_mut_batch_num != 0 else 0,\n",
    "                             \"data_perf_improvs\": data_perf_improvs, \"avg_data_improv\": float(data_perf_sum)/data_mut_batch_num if data_mut_batch_num != 0 else 0})\n",
    "    \n",
    "print(f'PERFORMANCE COMPARISON: {performance_logs}')\n",
    "with open(os.path.join(model_dir,'model_evaluation.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(performance_logs, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of Transformer Prediction Performance with Dataset Sequences on their respective Initial Conditions (Ground Truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8850/3658804447.py:182: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  decoded = tensor_decode(torch.tensor(token, dtype=torch.int8))\n",
      "/tmp/ipykernel_8850/3658804447.py:197: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  decoded = tensor_decode(torch.tensor(token, dtype=torch.int8))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERFORMANCE COMPARISON: [{'model_perf_improvs': [], 'avg_model_improv': 0.0, 'data_perf_improvs': [-0.009709132184854141, -0.027187078029807132, -0.01698703576466265, -0.01977577601934044, -0.0057668332343991935], 'avg_data_improv': -0.0007942585523306356}, {'model_perf_improvs': [-0.058499224260887495, -0.03221583141691298], 'avg_model_improv': -0.030238351892600157, 'data_perf_improvs': [-0.030642111941680206, -0.013590172932373967, -0.010283084896371975, -0.015728665641890238], 'avg_data_improv': -0.0007024403541231639}, {'model_perf_improvs': [], 'avg_model_improv': 0, 'data_perf_improvs': [-0.09795350108646605, -0.003244516229476524, -0.008993270224394223, -0.03082432757234166, -0.004169022046354165, -0.008820796777938789, -0.004346576711884431, -0.005795740649326753, -0.0039641393393736735], 'avg_data_improv': -0.0016811189063755628}, {'model_perf_improvs': [-0.055430474076423764, -0.05461823118713216, -0.062418944993305026], 'avg_model_improv': -0.04311691256421524, 'data_perf_improvs': [-0.03775022087773561, -0.047271379953486736, -0.007979110425067049, -0.005346143864556119, -0.00985761789092976, -0.013430588667289634, -0.02527425419452456], 'avg_data_improv': -0.0014690931587358945}, {'model_perf_improvs': [-0.029741765749662363, -0.041729272456338506, -0.03659499247724618], 'avg_model_improv': -0.018011005113874507, 'data_perf_improvs': [-0.011581645434358023, -0.030891979114801238, -0.038761154479463045, -0.00445803714280224], 'avg_data_improv': -0.0008569281617142455}, {'model_perf_improvs': [], 'avg_model_improv': 0.0, 'data_perf_improvs': [-0.0021244790295291693, -0.016665393507298454, -0.01301718073822844, -0.009664179720711985, -0.01927676648788168, -0.05651405992572123], 'avg_data_improv': -0.0011726205940937096}, {'model_perf_improvs': [-0.026666072436639432], 'avg_model_improv': -0.026666072436639432, 'data_perf_improvs': [-0.039485680742378015, -0.011236675573429444, -0.014203740226213846, -0.028877898205369235], 'avg_data_improv': -0.0009380399474739054}, {'model_perf_improvs': [-0.01077490105744161, -0.004576149259730045], 'avg_model_improv': -0.005117016772390552, 'data_perf_improvs': [-0.05488534961564806, -0.006148702950097729, -0.0017059377180599355, -0.019423831097634903, -0.02887099496287071, -0.012059031842163392, -0.00781324813350004], 'avg_data_improv': -0.0013090709631997477}, {'model_perf_improvs': [], 'avg_model_improv': 0, 'data_perf_improvs': [-0.08111152347374251], 'avg_data_improv': -0.0008111152347374251}, {'model_perf_improvs': [-0.014679654675192388, -0.0027738838992350523, -0.0001558000768451251], 'avg_model_improv': -0.0035218677302545132, 'data_perf_improvs': [-0.11017835047147706, -0.01409017713365568], 'avg_data_improv': -0.0012426852760513273}]\n"
     ]
    }
   ],
   "source": [
    "NUM_TEST_EXPERIMENTS = 10\n",
    "NUM_TEST_SEQUENCES = 10\n",
    "GRID_SIZE = 32\n",
    "RULESET_MUTATOR_CLASS = ArbitraryRulesetMutator\n",
    "RULE_SET = [conway(), seeds()]\n",
    "OPT_FUNC = surface_to_vol\n",
    "SRT_NUM_MUTATE = 10\n",
    "IC_NUM_MUTATE = 10\n",
    "RULESET_MUTE_PROB = 2/3\n",
    "\n",
    "# Measure the difference between the resulting performance metrics when applying the predicted mutations and running the automaton\n",
    "\n",
    "performance_logs = []\n",
    "\n",
    "for num_test in random.sample(range(len(encoded)), NUM_TEST_SEQUENCES):\n",
    "    # clear_initial()\n",
    "    # lattice = Lattice(dim=(GRID_SIZE, GRID_SIZE, GRID_SIZE))\n",
    "\n",
    "    ruleset_mutator_init = RULESET_MUTATOR_CLASS(rules=RULE_SET, grid_size=GRID_SIZE, mutate_p=1/(GRID_SIZE**2) * (SRT_NUM_MUTATE+IC_NUM_MUTATE), rule_mutate_p=RULESET_MUTE_PROB)\n",
    "    \n",
    "    optim_init = Optimizer(mutator=ruleset_mutator_init, objective=lambda grid: OPT_FUNC(grid))\n",
    "\n",
    "    optim_init.state.initial = initial_ics[num_test]\n",
    "    optim_init.state.rules = initial_srts[num_test]\n",
    "\n",
    "    model_perf_improvs = []\n",
    "    model_perf_sum = 0\n",
    "    model_mut_batch_num = 0\n",
    "    data_perf_improvs = []\n",
    "    data_perf_sum = 0\n",
    "    data_mut_batch_num = 0\n",
    "    \n",
    "    # test the mutation prediction model\n",
    "    for repeat_num in range(NUM_TEST_EXPERIMENTS):\n",
    "        optim = deepcopy(optim_init)\n",
    "        \n",
    "        curr_level = \"seq\" # seq, mutb, icm, srtm\n",
    "        curr_output = torch.full((1,max_seq_len), int(str_to_tensor(\"[PAD]\")[0]))\n",
    "        curr_idx = 1\n",
    "        in_idx = 0\n",
    "        curr_output[0][0] = str_to_tensor(\"[BOS]\")[0]\n",
    "        ic_mutations, srt_mutations = [], []\n",
    "        curr_mutation = None\n",
    "        it = 0\n",
    "\n",
    "        while curr_idx < max_seq_len:\n",
    "            with torch.no_grad():\n",
    "                next_output = model.net(curr_output.cuda())\n",
    "                # print(model.net.num_tokens)\n",
    "                # print(next_output.shape)\n",
    "                logits = next_output[0][curr_idx]\n",
    "                # print(logits)\n",
    "            valid = False\n",
    "            added_perf = False\n",
    "            while valid == False:\n",
    "                token = int(sample(logits)[0])\n",
    "                # print(token)\n",
    "                if curr_level == \"seq\":\n",
    "                    if token == int(str_to_tensor(\"[BMB]\")[0]):\n",
    "                        # print('STARTING MUTATION BATCH')\n",
    "                        curr_level = \"mutb\"\n",
    "                        valid = True\n",
    "                elif curr_level == \"mutb\":\n",
    "                    if token == int(str_to_tensor(\"[EMB]\")[0]):\n",
    "                        # perform mutations predicted & add the resulting performance to the sequence\n",
    "                        # print(f\"MUTATION BATCH {it}: IC: {ic_mutations}, SRT: {srt_mutations}\")\n",
    "                        old_objval = optim.objvalue\n",
    "                        accepted, new_state = optim.step_muts(ic_mutations, srt_mutations)\n",
    "                        if accepted:\n",
    "                            # print(\"Got a better state!\", optim.objvalue)\n",
    "                            model_perf_sum += optim.objvalue - old_objval\n",
    "                            model_perf_improvs.append(optim.objvalue - old_objval)\n",
    "                            # export(it + 1, lattice, export_dir)\n",
    "                        perf_tokens = str_to_tensor(f'[BP] {decimal_to_tokens(optim.objvalue)} [EP] ')[0]\n",
    "                        added_perf = True\n",
    "                        it += 1\n",
    "                        model_mut_batch_num += 1\n",
    "                        # end mutation batch\n",
    "                        curr_level = \"seq\"\n",
    "                        valid = True\n",
    "                        ic_mutations, srt_mutations = [], []\n",
    "                    elif token == int(str_to_tensor(\"[ICM]\")[0]):\n",
    "                        # got ic mutation\n",
    "                        curr_mutation = [0, 0]\n",
    "                        curr_level = \"icm\"\n",
    "                        valid = True\n",
    "                    elif token == int(str_to_tensor(\"[SRTM]\")[0]):\n",
    "                        # got srt mutation\n",
    "                        curr_mutation = [0, 0, 0]\n",
    "                        curr_level = \"srtm\"\n",
    "                        valid = True\n",
    "                elif curr_level == \"icm\":\n",
    "                    decoded = tensor_decode(torch.tensor(token, dtype=torch.int8))\n",
    "                    if decoded.isdigit() and int(decoded) < GRID_SIZE:\n",
    "                        # got ic dimension\n",
    "                        curr_mutation[in_idx] = int(decoded)\n",
    "                        in_idx += 1\n",
    "                        valid = True\n",
    "                        if in_idx == 2:\n",
    "                            in_idx = 0\n",
    "                            curr_level = \"mutb\"\n",
    "                            # print(f\"ICM: {curr_mutation}\")\n",
    "                            ic_mutations.append(curr_mutation)\n",
    "                            curr_mutation = None\n",
    "\n",
    "                elif curr_level == \"srtm\":\n",
    "                    decoded = tensor_decode(torch.tensor(token, dtype=torch.int8))\n",
    "                    if decoded.isdigit() and (in_idx > 0 or int(decoded) < GRID_SIZE-1):\n",
    "                        # got srt dimension\n",
    "                        curr_mutation[in_idx] = int(decoded)\n",
    "                        in_idx += 1\n",
    "                        valid = True\n",
    "                        if in_idx == 3:\n",
    "                            in_idx = 0\n",
    "                            curr_level = \"mutb\"\n",
    "                            # print(f\"SRTM: {curr_mutation}\")\n",
    "                            srt_mutations.append(curr_mutation)\n",
    "                            curr_mutation = None\n",
    "            curr_output[0][curr_idx] = token\n",
    "            curr_idx += 1\n",
    "            if added_perf and (curr_idx + perf_tokens.shape[0] <= 100):\n",
    "                for i in range(perf_tokens.shape[0]):\n",
    "                    curr_output[0][curr_idx] = perf_tokens[i]\n",
    "                    curr_idx += 1\n",
    "            \n",
    "    # test the dataset predictions\n",
    "    optim = deepcopy(optim_init)\n",
    "\n",
    "    curr_level = \"seq\" # seq, mutb, icm, srtm, perf\n",
    "    curr_idx = 1\n",
    "    in_idx = 0\n",
    "    ic_mutations, srt_mutations = [], []\n",
    "    curr_mutation = None\n",
    "    it = 0\n",
    "\n",
    "    while curr_idx < len(encoded[num_test]):\n",
    "        valid = False\n",
    "        token = encoded[num_test][curr_idx]\n",
    "        # print(tensor_decode(torch.tensor(token, dtype=torch.int8)))\n",
    "        if curr_level == \"seq\":\n",
    "            if token == int(str_to_tensor(\"[BMB]\")[0]):\n",
    "                # print('STARTING MUTATION BATCH')\n",
    "                curr_level = \"mutb\"\n",
    "                valid = True\n",
    "            elif token == int(str_to_tensor(\"[EOS]\")[0]):\n",
    "                break\n",
    "        elif curr_level == \"mutb\":\n",
    "            if token == int(str_to_tensor(\"[EMB]\")[0]):\n",
    "                # perform mutations predicted & add the resulting performance to the sequence\n",
    "                # print(f\"MUTATION BATCH {it}: IC: {ic_mutations}, SRT: {srt_mutations}\")\n",
    "                old_objval = optim.objvalue\n",
    "                accepted, new_state = optim.step_muts(ic_mutations, srt_mutations)\n",
    "                # print(f\"old obj: {old_objval}, new obj: {optim.objvalue}\")\n",
    "                if accepted:\n",
    "                    # print(\"Got a better state!\", optim.objvalue)\n",
    "                    data_perf_sum += optim.objvalue - old_objval\n",
    "                    data_perf_improvs.append(optim.objvalue - old_objval)\n",
    "                    # export(it + 1, lattice, export_dir)\n",
    "                it += 1\n",
    "                data_mut_batch_num += 1\n",
    "                # end mutation batch\n",
    "                curr_level = \"seq\"\n",
    "                valid = True\n",
    "                ic_mutations, srt_mutations = [], []\n",
    "            elif token == int(str_to_tensor(\"[ICM]\")[0]):\n",
    "                # got ic mutation\n",
    "                # print(\"GOT IC MUTATION\")\n",
    "                curr_mutation = [0, 0]\n",
    "                curr_level = \"icm\"\n",
    "                valid = True\n",
    "            elif token == int(str_to_tensor(\"[SRTM]\")[0]):\n",
    "                # got srt mutation\n",
    "                # print(\"GOT SRT MUTATION\")\n",
    "                curr_mutation = [0, 0, 0]\n",
    "                curr_level = \"srtm\"\n",
    "                valid = True\n",
    "            elif token == int(str_to_tensor(\"[BP]\")[0]):\n",
    "                # print(\"GOT PERF\")\n",
    "                curr_level = \"perf\"\n",
    "                valid = True\n",
    "        elif curr_level == \"icm\":\n",
    "            decoded = tensor_decode(torch.tensor(token, dtype=torch.int8))\n",
    "            if decoded.isdigit() and int(decoded) < GRID_SIZE:\n",
    "                # print(\"GOT IC DIM\")\n",
    "                # got ic dimension\n",
    "                curr_mutation[in_idx] = int(decoded)\n",
    "                in_idx += 1\n",
    "                valid = True\n",
    "                if in_idx == 2:\n",
    "                    in_idx = 0\n",
    "                    curr_level = \"mutb\"\n",
    "                    # print(f\"ICM: {curr_mutation}\")\n",
    "                    ic_mutations.append(curr_mutation)\n",
    "                    curr_mutation = None\n",
    "\n",
    "        elif curr_level == \"srtm\":\n",
    "            decoded = tensor_decode(torch.tensor(token, dtype=torch.int8))\n",
    "            if decoded.isdigit() and (in_idx > 0 or int(decoded) < GRID_SIZE-1):\n",
    "                # print(\"GOT SRT DIM\")\n",
    "                # got srt dimension\n",
    "                curr_mutation[in_idx] = int(decoded)\n",
    "                in_idx += 1\n",
    "                valid = True\n",
    "                if in_idx == 3:\n",
    "                    in_idx = 0\n",
    "                    curr_level = \"mutb\"\n",
    "                    # print(f\"SRTM: {curr_mutation}\")\n",
    "                    srt_mutations.append(curr_mutation)\n",
    "                    curr_mutation = None\n",
    "        \n",
    "        elif curr_level == \"perf\":\n",
    "            if token == int(str_to_tensor(\"[EP]\")[0]):\n",
    "                # print(\"ENDED PERF\")\n",
    "                curr_level = \"mutb\"\n",
    "            valid = True\n",
    "        assert valid, \"NOT VALID DATASET SEQUENCE\"\n",
    "        curr_idx += 1\n",
    "    \n",
    "    performance_logs.append({\"model_perf_improvs\": model_perf_improvs, \"avg_model_improv\": float(model_perf_sum)/model_mut_batch_num if model_mut_batch_num != 0 else 0,\n",
    "                             \"data_perf_improvs\": data_perf_improvs, \"avg_data_improv\": float(data_perf_sum)/data_mut_batch_num if data_mut_batch_num != 0 else 0})\n",
    "    \n",
    "print(f'PERFORMANCE COMPARISON: {performance_logs}')\n",
    "with open(os.path.join(model_dir,'model_comparison_ground_truth.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(performance_logs, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automata_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
